# ç¬¬5ç« ï¼šRunnable æ¥å£ä¸ä»»åŠ¡ç¼–æ’ç³»ç»Ÿ

# å‰è¨€
å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯é²«å°é±¼ã€‚æ˜¯ä¸€å`ä¸å†™å‰ç«¯ä»£ç `çš„å‰ç«¯å·¥ç¨‹å¸ˆï¼Œçƒ­è¡·äºåˆ†äº«éå‰ç«¯çš„çŸ¥è¯†ï¼Œå¸¦é¢†åˆ‡å›¾ä»”é€ƒç¦»åˆ‡å›¾åœˆå­ï¼Œæ¬¢è¿å…³æ³¨æˆ‘ï¼Œå¾®ä¿¡å…¬ä¼—å·ï¼š`ã€Šé²«å°é±¼ä¸æ­£ç»ã€‹`ã€‚æ¬¢è¿ç‚¹èµã€æ”¶è—ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿ï¼ï¼

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡
- å½»åº•ç†è§£ Runnable æŠ½è±¡ï¼š`invoke`ã€`batch`ã€`stream`ã€`pipe`
- ç†Ÿç»ƒä½¿ç”¨å¸¸è§å®ç°ï¼š`RunnableLambda`ã€`RunnableSequence`ã€`RunnableParallel`ã€`RunnablePassthrough`
- æŒæ¡ç¼–æ’æ¨¡å¼ï¼šé¡ºåºæµæ°´çº¿ã€æ¡ä»¶åˆ†æ”¯ã€æ‰‡å‡º/æ±‡èšã€Map/Reduceã€å›é€€ä¸é‡è¯•
- å°† Promptã€LLMã€Parserã€Retrieverã€å·¥å…·è°ƒç”¨æ•´åˆä¸ºå¯å¤ç”¨çš„å·¥ä½œæµ
- åœ¨ Next.js/Node ä¸­è½åœ°ä¸€å¥—å¯è§‚æµ‹ã€å¯æ‰©å±•ã€å¯æµ‹è¯•çš„ç¼–æ’å¼•æ“
- å®Œæˆä¸¤ä¸ªå®æˆ˜ï¼šå†…å®¹æ™ºèƒ½å¤„ç†æµæ°´çº¿ã€RAG æ•°æ®å¤„ç†æµæ°´çº¿

---

## ğŸ“– Runnable æ˜¯ä»€ä¹ˆ

### 5.1 æ ¸å¿ƒç†å¿µ
Runnable æ˜¯ LangChain.js çš„é€šç”¨å¯æ‰§è¡Œå•å…ƒæŠ½è±¡ã€‚å®ƒç»Ÿä¸€äº†â€œè¾“å…¥ â†’ å¤„ç† â†’ è¾“å‡ºâ€çš„æ¨¡å¼ï¼Œä½¿å¾— Promptã€æ¨¡å‹ã€è§£æå™¨ã€æ£€ç´¢å™¨ã€å·¥å…·ç”šè‡³ä½ è‡ªå®šä¹‰çš„å‡½æ•°ï¼Œéƒ½èƒ½ä»¥åŒä¸€å¥—æ¥å£è¿›è¡Œç»„åˆä¸ç¼–æ’ã€‚

å®ƒä¸æ˜¯â€œå¦ä¸€ä¸ªæ¡†æ¶å±‚â€ï¼Œè€Œæ˜¯â€œè®©ä½ çš„ä»£ç å¤©ç„¶å¯ç»„åˆã€å¯æµå¼ã€å¯æ‰¹å¤„ç†â€çš„ä¸€å±‚è–„æŠ½è±¡ã€‚

### 5.2 æ ‡å‡†æ¥å£
```typescript
interface Runnable<Input, Output> {
  invoke(input: Input, options?: RunnableConfig): Promise<Output>;
  stream(input: Input, options?: RunnableConfig): AsyncGenerator<Output>;
  batch(inputs: Input[], options?: RunnableConfig): Promise<Output[]>;
  pipe<NewOutput>(next: Runnable<Output, NewOutput>): Runnable<Input, NewOutput>;
}
```
- `invoke`ï¼šå•æ¬¡è°ƒç”¨
- `stream`ï¼šæµå¼äº§å‡ºï¼ˆå¦‚ token æµã€åˆ†ç‰‡ç»“æœï¼‰
- `batch`ï¼šæ‰¹å¤„ç†è¾“å…¥ï¼Œæé«˜åå
- `pipe`ï¼šå°†å½“å‰ Runnable çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ª Runnable çš„è¾“å…¥ï¼Œå½¢æˆé“¾å¼æµæ°´çº¿

### 5.3 å¸¸ç”¨å®ç°
- `RunnableLambda`ï¼šæŠŠä»»æ„å‡½æ•°åŒ…æˆ Runnable
- `RunnableSequence`ï¼šæŒ‰é¡ºåºå°†å¤šä¸ª Runnable ä¸²è”
- `RunnableParallel`ï¼šå¹¶è¡Œæ‰§è¡Œå¤šä¸ª Runnable å¹¶æ±‡æ€»
- `RunnablePassthrough`ï¼šåŸæ ·é€ä¼ è¾“å…¥ï¼Œé€šå¸¸é…åˆ `map` æ„å»ºå¤åˆå¯¹è±¡

> å¤‡æ³¨ï¼šä¸åŒç‰ˆæœ¬æ¥å£ç»†èŠ‚ç•¥æœ‰å‡ºå…¥ï¼Œæœ¬æ–‡ä»¥ LangChain.js 0.3 çš„ä¸»æµç”¨æ³•ä¸ºä¾‹ã€‚

---

## ğŸ§© ä» 0 åˆ° 1ï¼šæœ€å°å¯ç”¨æµæ°´çº¿

### 5.4 å°† Prompt â†’ æ¨¡å‹ â†’ è§£æå™¨ ä¸²èµ·æ¥
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/sequence-basic.ts
import { PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";

const prompt = PromptTemplate.fromTemplate(
  `ä½ æ˜¯{role}ï¼Œè¯·å¯¹ä¸‹é¢å†…å®¹ç»™å‡ºè¦ç‚¹æ€»ç»“ï¼š\n{content}`
);

const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 });
const parser = new StringOutputParser();

// ç»„åˆä¸ºå¤„ç†é“¾ï¼šInput -> Prompt -> LLM -> Parser -> Output(string)
const chain = prompt.pipe(model).pipe(parser);

export async function run() {
  const out = await chain.invoke({ role: "æŠ€æœ¯ä½œè€…", content: "React å¹¶å‘ç‰¹æ€§..." });
  console.log(out);
}

if (require.main === module) run();
```

### 5.5 ç”¨ RunnableLambda æ‰¿æ¥å¼‚æ„é€»è¾‘
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/lambda-basic.ts
import { RunnableLambda } from "@langchain/core/runnables";

// ç»Ÿä¸€è¾“å…¥è¾“å‡ºå½¢çŠ¶
type Input = { text: string };

const normalize = new RunnableLambda<Input, Input>((input) => ({
  text: input.text.trim().slice(0, 2000),
}));

export async function run() {
  const out = await normalize.invoke({ text: "   hello runnable    " });
  console.log(out); // { text: "hello runnable" }
}

if (require.main === module) run();
```

### 5.6 RunnableSequence.from([...]) çš„å¥½å¤„
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/sequence-from.ts
import { RunnableSequence, RunnableLambda } from "@langchain/core/runnables";

const trim = new RunnableLambda((x: string) => x.trim());
const exclaim = new RunnableLambda((x: string) => x + "!");

const seq = RunnableSequence.from<string, string>([
  trim,
  exclaim,
]);

export async function run() {
  console.log(await seq.invoke(" hello ")); // "hello!"
}

if (require.main === module) run();
```

---

## ğŸ”€ æ¡ä»¶åˆ†æ”¯ä¸æ‰‡å‡º/æ±‡èš

### 5.7 æ¡ä»¶åˆ†æ”¯ï¼ˆBranchï¼‰
æœ‰æ—¶æˆ‘ä»¬éœ€è¦æ ¹æ®è¾“å…¥å†…å®¹é€‰æ‹©ä¸åŒçš„å­é“¾ã€‚å¯ä»¥ç”¨ `RunnableLambda` å†™ä¸€ä¸ªè·¯ç”±å™¨æ¥è¿”å›ä¸åŒçš„ Runnableï¼š

```typescript
// æ–‡ä»¶ï¼šsrc/ch05/branch-router.ts
import { RunnableLambda } from "@langchain/core/runnables";

const toUpper = new RunnableLambda((x: string) => x.toUpperCase());
const toLower = new RunnableLambda((x: string) => x.toLowerCase());

// è·¯ç”±ï¼šåŒ…å«æ•°å­—èµ° Aï¼Œå¦åˆ™èµ° B
const router = new RunnableLambda<string, string>(async (input, config) => {
  const hasDigit = /\d/.test(input);
  const chosen = hasDigit ? toUpper : toLower;
  return chosen.invoke(input, config);
});

export async function run() {
  console.log(await router.invoke("Hello123")); // HELLO123
  console.log(await router.invoke("Hello"));    // hello
}

if (require.main === module) run();
```

> å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æä¾›æ¡ä»¶èŠ‚ç‚¹çš„æ›´é«˜çº§å°è£…ï¼Œä¹Ÿå¯ä»¥å®ç°æ›´ç›´è§‚çš„åˆ†æ”¯ï¼Œä½†ä¸Šé¢çš„çº¯ Runnable å†™æ³•çµæ´»ä¸”å¯æµ‹è¯•ã€‚

### 5.8 æ‰‡å‡º/æ±‡èšï¼ˆParallel/Fan-inï¼‰
å°†ä¸€ä¸ªè¾“å…¥åŒæ—¶äº¤ç»™å¤šä¸ªå¤„ç†å™¨ï¼Œæœ€åå†åˆå¹¶ç»“æœï¼š

```typescript
// æ–‡ä»¶ï¼šsrc/ch05/parallel-basic.ts
import { RunnableParallel, RunnableLambda } from "@langchain/core/runnables";

const a = new RunnableLambda((x: string) => `A:${x.length}`);
const b = new RunnableLambda((x: string) => `B:${x.split(" ").length}`);
const c = new RunnableLambda((x: string) => `C:${x.includes("AI")}`);

const parallel = new RunnableParallel({ a, b, c });

const merge = new RunnableLambda((out: { a: string; b: string; c: string }) =>
  `${out.a} | ${out.b} | ${out.c}`
);

export async function run() {
  const res = await parallel.pipe(merge).invoke("AI makes front-end better");
  console.log(res);
}

if (require.main === module) run();
```

---

## ğŸ“¦ å¤åˆå¯¹è±¡ä¸ Passthrough/Map æ¨¡å¼

### 5.9 è®©é“¾è·¯åŒæ—¶æºå¸¦å¤šä¸ªå­—æ®µ
`RunnablePassthrough` å¯æŠŠä¸Šæ¸¸è¾“å…¥åŸæ ·é€ä¼ ï¼Œé…åˆå¯¹è±¡ç»“æ„è½»æ¾ç»„è£…å¤æ‚è¾“å…¥ï¼š

```typescript
// æ–‡ä»¶ï¼šsrc/ch05/passthrough-map.ts
import { RunnableLambda, RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables";

const tokenize = new RunnableLambda((x: string) => x.split(/\s+/));
const count = new RunnableLambda((xs: string[]) => xs.length);

// äº§å‡ºå½¢å¦‚ { raw, tokens, count }
const pipeline = RunnableSequence.from([
  new RunnableLambda((raw: string) => ({ raw })),
  new RunnableLambda(async ({ raw }) => ({ raw, tokens: await tokenize.invoke(raw) })),
  new RunnableLambda(async ({ raw, tokens }) => ({ raw, tokens, count: await count.invoke(tokens) })),
  RunnablePassthrough.from(),
]);

export async function run() {
  const out = await pipeline.invoke("hello runnable world");
  console.log(out);
}

if (require.main === module) run();
```

---

## ğŸš° æµå¼å¤„ç†ä¸èšåˆ

### 5.10 å°†æµå¼è¾“å‡ºèšåˆä¸ºä¸€æ®µæ–‡æœ¬
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/stream-collect.ts
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo", streaming: true });

export async function run() {
  const stream = await model.stream("è¯·ç”¨ 5 å¥ä»‹ç» Runnable çš„ä¼˜åŠ¿");
  let acc = "";
  for await (const chunk of stream) {
    process.stdout.write(chunk.content);
    acc += chunk.content;
  }
  console.log("\n---\næ±‡æ€»ï¼š\n", acc);
}

if (require.main === module) run();
```

### 5.11 æµæ°´çº¿ä¸­çš„æµå¼ç‰‡æ®µ
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/stream-transform.ts
import { ChatOpenAI } from "@langchain/openai";

export async function run() {
  const llm = new ChatOpenAI({ streaming: true });
  const stream = await llm.stream("åˆ†ç‚¹è¯´æ˜ Runnable çš„ 4 ä¸ªå…³é”®æ–¹æ³•");
  let i = 0;
  for await (const chunk of stream) {
    console.log(`[${i++}]`, chunk.content);
  }
}

if (require.main === module) run();
```

---

## ğŸ§± ç¨³å¥æ€§ï¼šé”™è¯¯ã€é‡è¯•ä¸å›é€€

### 5.12 é“¾è·¯çº§é”™è¯¯å¤„ç†
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/errors-basic.ts
import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";

const parseIntSafe = new RunnableLambda((x: string) => {
  const n = Number.parseInt(x, 10);
  if (Number.isNaN(n)) throw new Error("ä¸æ˜¯åˆæ³•æ•´æ•°");
  return n;
});

const square = new RunnableLambda((n: number) => n * n);

const seq = RunnableSequence.from([parseIntSafe, square]);

export async function run() {
  try {
    console.log(await seq.invoke("16"));
    console.log(await seq.invoke("oops"));
  } catch (e) {
    console.error("å¤±è´¥ï¼š", (e as Error).message);
  }
}

if (require.main === module) run();
```

### 5.13 å›é€€ç­–ç•¥ï¼ˆFallbackï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/fallback.ts
import { RunnableLambda } from "@langchain/core/runnables";

const primary = new RunnableLambda((x: string) => {
  if (x.length < 5) throw new Error("å¤ªçŸ­");
  return x.toUpperCase();
});

const fallback = new RunnableLambda((x: string) => `[fallback] ${x}`);

async function withFallback(x: string) {
  try {
    return await primary.invoke(x);
  } catch {
    return await fallback.invoke(x);
  }
}

export async function run() {
  console.log(await withFallback("hello"));
  console.log(await withFallback("hi"));
}

if (require.main === module) run();
```

### 5.14 ç®€å•é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/retry.ts
import { RunnableLambda } from "@langchain/core/runnables";

const flaky = new RunnableLambda(async (n: number) => {
  if (Math.random() < 0.7) throw new Error("ä¸´æ—¶é”™è¯¯");
  return n * 2;
});

async function retry<T>(fn: (x: T) => Promise<T>, x: T, times = 3) {
  let attempt = 0;
  while (attempt < times) {
    try { return await fn(x); } catch (e) {
      await new Promise(r => setTimeout(r, 2 ** attempt * 200));
      attempt++;
    }
  }
  throw new Error("é‡è¯•å¤±è´¥");
}

export async function run() {
  const out = await retry((x) => flaky.invoke(x), 10, 4);
  console.log(out);
}

if (require.main === module) run();
```

---

## ğŸ§ª æ‰¹å¤„ç†ä¸å¹¶è¡Œåº¦

### 5.15 batch æå‡åå
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/batch.ts
import { RunnableLambda } from "@langchain/core/runnables";

const upper = new RunnableLambda((s: string) => s.toUpperCase());

export async function run() {
  const inputs = ["a", "b", "c", "d"];
  const outs = await upper.batch(inputs);
  console.log(outs);
}

if (require.main === module) run();
```

### 5.16 RunnableParallel æ§åˆ¶æ‰‡å‡ºè§„æ¨¡
```typescript
// æ–‡ä»¶ï¼šsrc/ch05/parallel-limit.ts
import { RunnableParallel, RunnableLambda } from "@langchain/core/runnables";

const slow = new RunnableLambda(async (s: string) => {
  await new Promise(r => setTimeout(r, 200));
  return s.repeat(2);
});

const parallel = new RunnableParallel({ a: slow, b: slow, c: slow });

export async function run() {
  console.time("p");
  console.log(await parallel.invoke("x"));
  console.timeEnd("p");
}

if (require.main === module) run();
```

---

## ğŸ—ï¸ ç»„åˆå®è·µï¼šæ„å»ºå¯å¤ç”¨å·¥ä½œæµ

### 5.17 å†…å®¹æ™ºèƒ½å¤„ç†æµæ°´çº¿ï¼ˆæ¸…æ´— â†’ è¯†åˆ« â†’ ç¿»è¯‘ â†’ æ€»ç»“ â†’ åˆ†ç±» â†’ ç»“æ„åŒ–ï¼‰
ç›®æ ‡ï¼šç»™å®šç”¨æˆ·è¾“å…¥çš„ä»»æ„æ–‡æœ¬ï¼Œå®Œæˆï¼š
- é¢„å¤„ç†æ¸…æ´—ï¼ˆå»å™ª/æˆªæ–­/å®‰å…¨ï¼‰
- è¯­è¨€è¯†åˆ«
- éœ€è¦æ—¶ç¿»è¯‘ä¸ºä¸­æ–‡
- æ‘˜è¦ç”Ÿæˆ
- é£æ ¼åˆ†ç±»ä¸æ ‡ç­¾æŠ½å–ï¼ˆå¹¶è¡Œï¼‰
- ç»“æ„åŒ–è¾“å‡º JSONï¼Œä¾¿äºå‰ç«¯ç›´æ¥æ¸²æŸ“

```typescript
// æ–‡ä»¶ï¼šsrc/ch05/pipeline-content.ts
import { RunnableLambda, RunnableParallel, RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { JsonOutputParser } from "@langchain/core/output_parsers";

type Input = { text: string };

const sanitize = new RunnableLambda<Input, Input>(({ text }) => ({
  text: text.replace(/\s+/g, " ").trim().slice(0, 4000),
}));

const detectPrompt = PromptTemplate.fromTemplate(
  `åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬çš„è¯­è¨€ï¼š\n{txt}\nåªè¿”å›è¯­è¨€åç§°ï¼Œå¦‚ Chinese/English/Japanese`
);
const llm = new ChatOpenAI({ temperature: 0 });
const detect = detectPrompt.pipe(llm);

const translatePrompt = PromptTemplate.fromTemplate(
  `å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š\n{txt}`
);
const translate = translatePrompt.pipe(llm);

const summaryPrompt = PromptTemplate.fromTemplate(
  `ç”¨è¦ç‚¹æ€»ç»“ä¸‹é¢æ–‡æœ¬ï¼ˆæœ€å¤š 5 æ¡ï¼‰ï¼š\n{txt}`
);
const summary = summaryPrompt.pipe(llm);

const stylePrompt = PromptTemplate.fromTemplate(
  `åˆ¤æ–­æ–‡æœ¬é£æ ¼æ ‡ç­¾ï¼ˆæŠ€æœ¯/è¥é”€/æ–°é—»/éšç¬”/å…¶ä»–ï¼‰ï¼š\n{txt}\nåªè¾“å‡ºä¸€ä¸ªæ ‡ç­¾`
);
const tagsPrompt = PromptTemplate.fromTemplate(
  `ä»æ–‡æœ¬ä¸­æŠ½å– 3 ä¸ªå…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰ï¼š\n{txt}\nä»¥é€—å·åˆ†éš”`
);

const parallelClassify = new RunnableParallel({
  style: stylePrompt.pipe(llm),
  tags: tagsPrompt.pipe(llm),
});

const pack = new RunnableLambda(async (ctx: {
  original: string; lang: string; textZh: string; summary: string; style: any; tags: any;
}) => {
  return {
    lang: ctx.lang.trim(),
    text: ctx.textZh.trim(),
    summary: ctx.summary.trim(),
    style: String(ctx.style.content || ctx.style).trim(),
    tags: String(ctx.tags.content || ctx.tags)
      .split(/[,ï¼Œ]/).map(s => s.trim()).filter(Boolean).slice(0, 5),
    original: ctx.original,
  };
});

const schemaPrompt = PromptTemplate.fromTemplate(
  `å°†ä»¥ä¸‹å¯¹è±¡é‡æ–°ç»„ç»‡ä¸ºä¸¥æ ¼ JSONï¼ˆkeys: lang,text,summary,style,tags,originalï¼‰ï¼š\n{obj}`
);
const parser = new JsonOutputParser<any>();
const schema = schemaPrompt.pipe(llm).pipe(parser);

export const contentPipeline = RunnableSequence.from([
  new RunnableLambda((input: Input) => ({ original: input.text })),
  new RunnableLambda(async ({ original }) => ({ original, cleaned: await sanitize.invoke({ text: original }) })),
  new RunnableLambda(async ({ original, cleaned }) => ({ original, cleaned, lang: (await detect.invoke({ txt: cleaned.text })).content })),
  new RunnableLambda(async ({ original, cleaned, lang }) => ({
    original,
    lang,
    textZh: lang.toLowerCase().startsWith("chinese") ? cleaned.text : (await translate.invoke({ txt: cleaned.text })).content,
  })),
  new RunnableLambda(async ({ original, lang, textZh }) => ({ original, lang, textZh, summary: (await summary.invoke({ txt: textZh })).content })),
  new RunnableLambda(async ({ original, lang, textZh, summary }) => ({
    original, lang, textZh, summary,
    classify: await parallelClassify.invoke({ txt: textZh }),
  })),
  new RunnableLambda(({ original, lang, textZh, summary, classify }) => ({
    original, lang, textZh, summary,
    style: classify.style, tags: classify.tags,
  })),
  pack,
  new RunnableLambda(async (obj) => schema.invoke({ obj: JSON.stringify(obj, null, 2) })),
]);

export async function run() {
  const input = { text: "LangChain unifies prompts, LLMs, retrievers into composable pipelines." };
  const out = await contentPipeline.invoke(input);
  console.log(out);
}

if (require.main === module) run();
```

### 5.18 API é›†æˆä¸å‰ç«¯æ¸²æŸ“
- Next.js API Route æ¥æ”¶ `text`ï¼Œè°ƒç”¨ `contentPipeline.invoke`
- å‰ç«¯ç›´æ¥æ¸²æŸ“ JSON å­—æ®µï¼ˆ`summary`ã€`tags`ã€`style`ï¼‰ï¼Œå¹¶æ˜¾ç¤º `original`
- ç»“åˆ Callback/æ—¥å¿—è®°å½•å¤„ç†æ—¶é—´ã€å¤±è´¥ç‡ã€token ä½¿ç”¨

---

## ğŸ§± å®æˆ˜ï¼šRAG æ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆETLï¼‰
ç›®æ ‡ï¼šå°†åŸå§‹æ–‡æ¡£æ¸…æ´—ã€åˆ‡åˆ†ã€åµŒå…¥ã€å†™å…¥å‘é‡åº“ï¼Œå¹¶è¾“å‡ºå…¥åº“ç»Ÿè®¡æŠ¥è¡¨ã€‚

æµç¨‹ï¼š
1) è¯»å–æ–‡ä»¶ â†’ 2) æ¸…æ´—ä¸åˆ†å— â†’ 3) è¿‡æ»¤çŸ­ç‰‡æ®µä¸é‡å¤ â†’ 4) å¹¶è¡ŒåµŒå…¥ â†’ 5) å†™å…¥å‘é‡åº“ â†’ 6) ç”ŸæˆæŠ¥å‘Š

```typescript
// æ–‡ä»¶ï¼šsrc/ch05/rag-etl.ts
import { RunnableLambda, RunnableParallel, RunnableSequence } from "@langchain/core/runnables";

// ä¼ªå®ç°ï¼šçœŸå®é¡¹ç›®è¯·æ›¿æ¢ä¸ºå„è‡ª Loader/Embedding/VectorStore
async function loadFiles(glob: string): Promise<{ id: string; text: string }[]> {
  return [
    { id: "a", text: "LangChain.js æ˜¯æ„å»º LLM åº”ç”¨çš„ JS æ¡†æ¶" },
    { id: "b", text: "Runnable æä¾›ç»Ÿä¸€çš„ invoke/stream/batch æ¥å£" },
  ];
}

async function split(docs: { id: string; text: string }[]) {
  return docs.flatMap(d => {
    const parts = d.text.match(/.{1,20}/g) || [];
    return parts.map((t, i) => ({ id: `${d.id}-${i}`, text: t }));
  });
}

async function dedup(chunks: { id: string; text: string }[]) {
  const seen = new Set<string>();
  return chunks.filter(c => { const k = c.text.trim(); if (seen.has(k)) return false; seen.add(k); return true; });
}

async function embedBatch(texts: string[]) { return texts.map(t => ({ vector: Array(8).fill(0).map((_,i)=> (t.length*(i+1))%7) })); }
async function upsertVectors(items: { id: string; vector: number[] }[]) { return items.length; }

const pipeline = RunnableSequence.from([
  new RunnableLambda((input: { glob: string }) => input.glob),
  new RunnableLambda(async (glob) => await loadFiles(glob)),
  new RunnableLambda(async (docs) => await split(docs)),
  new RunnableLambda(async (chunks) => chunks.filter(c => c.text.trim().length >= 4)),
  new RunnableLambda(async (chunks) => await dedup(chunks)),
  new RunnableLambda(async (chunks: { id: string; text: string }[]) => {
    const textList = chunks.map(c => c.text);
    const parallel = new RunnableParallel({
      vectors: new RunnableLambda(async () => embedBatch(textList)),
      ids: new RunnableLambda(async () => chunks.map(c => c.id)),
    });
    const { vectors, ids } = await parallel.invoke(undefined as any);
    return ids.map((id, i) => ({ id, vector: vectors[i].vector }));
  }),
  new RunnableLambda(async (items) => ({ upserted: await upsertVectors(items), total: items.length })),
]);

export async function run() {
  const report = await pipeline.invoke({ glob: "docs/**/*.md" });
  console.log("æŠ¥å‘Šï¼š", report);
}

if (require.main === module) run();
```

è¦ç‚¹ï¼š
- æŠŠâ€œé•¿é“¾è·¯â€æ‹†åˆ†æˆè‹¥å¹²å°çš„ Runnableï¼Œä¾¿äºæµ‹è¯•å’Œå¤ç”¨
- æ‰¹é‡åµŒå…¥ä¸å¹¶è¡Œ upsert å¯æ˜¾è‘—æå‡åå
- äº§å‡ºç»“æ„åŒ–â€œæŠ¥å‘Šâ€ï¼Œä¾¿äºæ¥å…¥ç›‘æ§/å‘Šè­¦/çœ‹æ¿

---

## ğŸŒ ä¸ Next.js çš„å·¥ç¨‹åŒ–è½åœ°

### 5.19 API è®¾è®¡å»ºè®®
- è¾“å…¥è¾“å‡ºçš†ä¸ºç»“æ„åŒ– JSONï¼Œå‰ç«¯æ¸²æŸ“æ— éœ€å†è§£æè‡ªç„¶è¯­è¨€
- æ”¯æŒ `stream` æ¨¡å¼ï¼Œå‰ç«¯å®ç°æ‰“å­—æœº/è¿›åº¦æ¡
- çº¦å®š `requestId`ï¼Œè´¯ç©¿æ—¥å¿—ä¸åŸ‹ç‚¹

### 5.20 å‰ç«¯ä½“éªŒ
- æäº¤åç«‹å³æ˜¾ç¤ºâ€œä»»åŠ¡é˜Ÿåˆ—ä¸­/å¼€å§‹å¤„ç†/å¤„ç†å®Œæˆâ€ä¸‰æ€
- æµå¼å±•ç¤ºâ€œé˜¶æ®µæ€§äº§å‡ºâ€ï¼šå¦‚å…ˆå‡ºæ‘˜è¦ï¼Œå†å‡ºæ ‡ç­¾ï¼Œå†å‡ºå¼•ç”¨
- å¤±è´¥æ—¶ä¿ç•™æœ€åä¸€æ¬¡æˆåŠŸè¾“å‡ºï¼Œå…è®¸â€œç»§ç»­/é‡è¯•/åé¦ˆâ€

---

## ğŸ” å¯è§‚æµ‹æ€§ä¸è°ƒè¯•

### 5.21 æ—¥å¿—ä¸å›è°ƒ
- ä¸ºå…³é”® Runnable æ³¨å…¥å›è°ƒï¼Œæ‰“å°è¾“å…¥é•¿åº¦ã€token æˆæœ¬ã€è€—æ—¶
- Runnable çš„ `tags/metadata` ç”¨äºèšåˆæŠ¥è¡¨

### 5.22 æ–­ç‚¹ä¸æœ€å°åŒ–é‡ç°
- å°†é•¿é“¾æ‹†åˆ†ä¸ºå¤šæ–‡ä»¶/å¤š Runnableï¼Œå•å…ƒæµ‹è¯•é€æ®µéªŒè¯
- æœ€å°åŒ–è¾“å…¥å¤ç°å®ä¾‹ï¼Œä¾¿äºæäº¤ issue æˆ–å¤ç›˜

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### 5.23 å•å…ƒæµ‹è¯•
- å¯¹çº¯å‡½æ•°å‹ `RunnableLambda` åšè¾“å…¥ â†’ è¾“å‡ºæ–­è¨€
- å¯¹é“¾è·¯ç»“æ„æ–­è¨€ï¼šç¡®ä¿é¡ºåº/åˆ†æ”¯/å¹¶è¡ŒæŒ‰é¢„æœŸæ‰§è¡Œ

### 5.24 é›†æˆæµ‹è¯•
- ä½¿ç”¨å›ºå®šæ ·ä¾‹ï¼ˆgolden setï¼‰ï¼Œå¯¹ä¸»è¦æµæ°´çº¿åšå›å½’
- ä¸º LLM èŠ‚ç‚¹æ‰“æ¡©ï¼ˆmockï¼‰ï¼Œåªæµ‹è¯•ç¼–æ’é€»è¾‘

---

## âš™ï¸ æ€§èƒ½ä¸æˆæœ¬å»ºè®®
- å°½é‡å¤ç”¨ Prompt ä¸æ¨¡å‹å®ä¾‹ï¼Œå‡å°‘å¯¹è±¡åˆ›å»º
- åˆç†é€‰æ‹©å¹¶è¡Œåº¦ï¼›I/O å¯†é›†å‹èŠ‚ç‚¹å¯å¹¶è¡Œï¼ŒLLM èŠ‚ç‚¹å—é™äºé…é¢/æˆæœ¬
- æ§åˆ¶è¾“å‡ºé•¿åº¦ã€å‹ç¼©ä¸­é—´æ€ï¼Œå‡å°‘ token
- å¼•å…¥ç¼“å­˜ï¼šå¯¹æ˜‚è´µèŠ‚ç‚¹åšé”®æ§ç¼“å­˜ï¼ˆå¦‚è¾“å…¥ hash â†’ è¾“å‡ºï¼‰

---

## ğŸ” å®‰å…¨ä¸å¥å£®æ€§
- è¿‡æ»¤ä¸é€ƒé€¸ï¼šå¯¹ç”¨æˆ·è¾“å…¥åšæ¸…æ´—ï¼Œé¿å… prompt æ³¨å…¥
- å¤±è´¥å›é€€ï¼šä¸ºå…³é”®ç¯èŠ‚å‡†å¤‡é™çº§ç­–ç•¥
- å®¡è®¡ä¸è¿½è¸ªï¼šä¿ç•™è¯·æ±‚é“¾è·¯ä¸å…³é”®ä¸­é—´ç»“æœï¼Œä¾¿äºäº‹åå®¡æŸ¥

---

## ğŸ“š å»¶ä¼¸é˜…è¯»ä¸èµ„æº
- LangChain.jsï¼ˆJSï¼‰æ–‡æ¡£ï¼š`https://js.langchain.com/`
- Runnable æŒ‡å—ä¸ç¤ºä¾‹ï¼šå®˜æ–¹æ•™ç¨‹ä¸­çš„ Runnable ç« èŠ‚
- æµå¼å¤„ç†ä¸ SSEï¼šMDN ä¸ Next.js Route Handlers æ–‡æ¡£
- æ€§èƒ½ä¸æˆæœ¬ï¼šæ¨¡å‹é€‰æ‹©ã€batchã€å¹¶è¡Œä¸ç¼“å­˜

---

## âœ… æœ¬ç« å°ç»“
- æŒæ¡äº† Runnable çš„æ ¸å¿ƒèƒ½åŠ›ä¸ç»„åˆæ–¹å¼
- èƒ½å°† Prompt/LLM/Parser/Tool/Retriever ç¼–æ’ä¸ºç¨³å®šçš„å·¥ä½œæµ
- å­¦ä¼šäº†é”™è¯¯å¤„ç†ã€å›é€€ä¸ç®€å•é‡è¯•
- å®Œæˆå†…å®¹å¤„ç†ä¸ RAG ETL ä¸¤ä¸ªå®Œæ•´æµæ°´çº¿

## ğŸ¯ ä¸‹ç« é¢„å‘Š
ä¸‹ä¸€ç« ã€ŠVector å‘é‡åŒ–æŠ€æœ¯ä¸è¯­ä¹‰æœç´¢ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- æ·±å…¥ Embedding ä¸å‘é‡ç›¸ä¼¼åº¦çš„åŸç†ä¸å®è·µ
- å¯¹æ¯” Chroma/Pinecone/Weaviateï¼Œå¹¶åŠ¨æ‰‹å®ç°æ£€ç´¢å™¨
- æ„å»ºæ··åˆæœç´¢ä¸é‡æ’åºï¼Œæ‰“é€š RAG çš„å…³é”®è·¯å¾„

> æœ€åæ„Ÿè°¢é˜…è¯»ï¼æ¬¢è¿å…³æ³¨æˆ‘ï¼Œå¾®ä¿¡å…¬ä¼—å·ï¼š`ã€Šé²«å°é±¼ä¸æ­£ç»ã€‹`ã€‚æ¬¢è¿ç‚¹èµã€æ”¶è—ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿ï¼ï¼ï¼

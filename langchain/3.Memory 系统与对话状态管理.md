# ç¬¬3ç« ï¼šMemory ç³»ç»Ÿä¸å¯¹è¯çŠ¶æ€ç®¡ç†

# å‰è¨€
å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯é²«å°é±¼ã€‚æ˜¯ä¸€å`ä¸å†™å‰ç«¯ä»£ç `çš„å‰ç«¯å·¥ç¨‹å¸ˆï¼Œçƒ­è¡·äºåˆ†äº«éå‰ç«¯çš„çŸ¥è¯†ï¼Œå¸¦é¢†åˆ‡å›¾ä»”é€ƒç¦»åˆ‡å›¾åœˆå­ï¼Œæ¬¢è¿å…³æ³¨æˆ‘ï¼Œå¾®ä¿¡å…¬ä¼—å·ï¼š`ã€Šé²«å°é±¼ä¸æ­£ç»ã€‹`ã€‚æ¬¢è¿ç‚¹èµã€æ”¶è—ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿ï¼ï¼

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡
- ç³»ç»Ÿç†è§£ LangChain.js çš„ Memory ä½“ç³»ä¸é€‚ç”¨è¾¹ç•Œ
- æŒæ¡çŸ­æœŸ/é•¿æœŸ/æ‘˜è¦/å‘é‡è®°å¿†ç­‰å¤šç§æ–¹æ¡ˆçš„å–èˆä¸ç»„åˆ
- èƒ½ç”¨ `MessagesPlaceholder` å°†å†å²å¯¹è¯æ³¨å…¥ Promptï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿ç»­
- å­¦ä¼šå°† Memory æŒä¹…åŒ–åˆ° Redis/MongoDBï¼Œå¹¶å¤„ç†å¹¶å‘ã€è¿‡æœŸä¸å‹ç¼©
- å°† Memory ä¸ Runnableã€Callbackã€LangGraph çŠ¶æ€å›¾ç»“åˆï¼Œæ‰“é€ å¯è§‚æµ‹çš„å¯¹è¯ç³»ç»Ÿ
- å®æˆ˜ 2 ä¸ªé¡¹ç›®ï¼šå¤šç”¨æˆ·ä¼šè¯ä¸­å¿ƒã€ä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹ï¼ˆé•¿æœŸè®°å¿†ï¼‰

---

## ğŸ“– Memory ç†è®ºä¸æ¶æ„ï¼ˆçº¦ 30%ï¼‰

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦ Memory
- è¯­è¨€æ¨¡å‹æœ¬èº«æ˜¯â€œæ— çŠ¶æ€â€çš„ï¼Œæ¯æ¬¡è°ƒç”¨åªä¾èµ–è¾“å…¥ Prompt
- å¯¹è¯å‹åº”ç”¨éœ€è¦â€œä¸Šä¸‹æ–‡è®°å¿†â€ï¼Œè®©æ¨¡å‹ç†è§£â€œæˆ‘ä»¬åˆšèŠåˆ°å“ªå„¿äº†â€
- è®°å¿†çš„æœ¬è´¨ï¼šåœ¨å¤šè½®å¯¹è¯é—´ä¼ é€’â€œå‹ç¼©è¿‡çš„è¯­ä¹‰â€ä¸â€œå…³é”®äº‹å®â€

### 3.2 å¸¸è§ Memory ç±»å‹ä¸æƒè¡¡
- Bufferï¼ˆå¯¹è¯ç¼“å†²ï¼‰
  - å…¨é‡ä¿ç•™è¿‘å‡ è½®æ¶ˆæ¯ï¼Œç®€å•ç›´æ¥
  - é£é™©ï¼štoken è†¨èƒ€ã€æˆæœ¬å‡é«˜ã€å“åº”å˜æ…¢
- Buffer Windowï¼ˆæ»‘åŠ¨çª—å£ï¼‰
  - ä»…ä¿ç•™æœ€è¿‘ N æ¡ï¼Œé™ä½ token
  - é£é™©ï¼šå¿˜è®°æ—©æœŸä½†ä»é‡è¦çš„ä¿¡æ¯
- Summaryï¼ˆæ‘˜è¦è®°å¿†ï¼‰
  - ç”¨æ¨¡å‹å°†å†å²å‹ç¼©æˆâ€œæ‘˜è¦â€ï¼Œå†ä¸çŸ­æœŸä¸Šä¸‹æ–‡æ‹¼æ¥
  - é£é™©ï¼šæ‘˜è¦åå·®ã€ä¿¡æ¯ä¸¢å¤±ï¼›éœ€è®¾è®¡â€œå†æ ¡å‡†/å†æ£€ç´¢â€æœºåˆ¶
- Vector Store Memoryï¼ˆå‘é‡è®°å¿†ï¼‰
  - å°†å¯¹è¯äº‹å®å‘é‡åŒ–ï¼ŒæŒ‰éœ€æ£€ç´¢ï¼ˆè¿‘ä¼¼è¯­ä¹‰åŒ¹é…ï¼‰
  - é£é™©ï¼šå¬å›è¯¯å·®ã€ç›¸ä¼¼åº¦é˜ˆå€¼é€‰æ‹©ã€å‘é‡åº“è¿ç»´æˆæœ¬

### 3.3 è®¾è®¡ç›®æ ‡ä¸æŒ‡æ ‡
- è¿ç»­æ€§ï¼šå‰åè¯­ä¹‰ä¸€è‡´ã€ä¸Šä¸‹æ–‡è¿è´¯
- ç»æµæ€§ï¼štoken æˆæœ¬ã€å»¶è¿Ÿã€å¸¦å®½
- ç¨³å¥æ€§ï¼šé‡å¤ä¿¡æ¯å»é‡ã€å†²çªä¸å¹»è§‰æ§åˆ¶
- å®‰å…¨æ€§ï¼šæ•æ„Ÿæ•°æ®è„±æ•ã€æœ€å°åŒ–ä¿ç•™ç­–ç•¥ã€è®¿é—®æ§åˆ¶

### 3.4 å…¸å‹æ¶æ„å›¾ï¼ˆæ¦‚å¿µï¼‰
```
ç”¨æˆ·è¯·æ±‚ â†’ ä¼šè¯æ§åˆ¶å™¨(Session) â†’ Memory ç®¡ç†å™¨
            â”œâ”€ çŸ­æœŸï¼šBuffer/Window
            â”œâ”€ é•¿æœŸï¼šSummary/Vector
            â”œâ”€ æŒä¹…åŒ–ï¼šRedis/Mongo
            â””â”€ è§‚æµ‹ï¼šæ—¥å¿—/å›æ”¾/è¯„åˆ†
â†’ Prompt æ¨¡æ¿ï¼ˆMessagesPlaceholder æ³¨å…¥ï¼‰â†’ æ¨¡å‹ â†’ è¾“å‡º â†’ å›å†™ Memory
```

---

## ğŸ§© LangChain.js Memory ç”Ÿæ€ï¼ˆçº¦ 10%ï¼‰

> æ³¨ï¼šæœ¬ç« ç¤ºä¾‹åŸºäº LangChain.js æœ€æ–°æ¥å£é£æ ¼ï¼Œéƒ¨åˆ† Memory å®ç°ä½äº `@langchain/community`ï¼ˆæˆ– `langchain/memory`ï¼‰ã€‚å¦‚æœ‰åŒ…è·¯å¾„å·®å¼‚ï¼Œè¯·ä»¥æœ¬åœ°ç‰ˆæœ¬ä¸ºå‡†ã€‚

- `ConversationBufferMemory`
- `ConversationBufferWindowMemory`
- `ConversationSummaryMemory`
- `VectorStoreRetrieverMemory`
- è‡ªå®šä¹‰ Memoryï¼ˆå®ç° `loadMemoryVariables` / `saveContext` æ¥å£ï¼‰

---

## ğŸ’» åŸºç¡€åˆ°è¿›é˜¶ä»£ç ï¼ˆçº¦ 40%ï¼‰

### 3.5 ç”¨ MessagesPlaceholder æ³¨å…¥å†å²
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/basic-placeholder.ts
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ç®€æ´çš„å‰ç«¯é¡¾é—®ã€‚"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

const model = new ChatOpenAI({ temperature: 0 });
const chain = prompt.pipe(model).pipe(new StringOutputParser());

// ç”±å¤–éƒ¨æ³¨å…¥ historyï¼ˆå¯æ¥è‡ª Memoryï¼‰
export async function run(history: any[], input: string) {
  const out = await chain.invoke({ history, input });
  console.log(out);
}

if (require.main === module) {
  run([{ role: "human", content: "æˆ‘ä»¬åˆšè®¨è®ºäº†é¦–å±ä¼˜åŒ–" }], "ç»§ç»­è¯´è¯´å›¾ç‰‡ä¼˜åŒ–");
}
```

### 3.6 Buffer ä¸ Windowï¼šæƒè¡¡ç¤ºä¾‹
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/window-buffer.ts
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { ConversationBufferWindowMemory } from "langchain/memory";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ä¸“ä¸šçš„æ€§èƒ½ä¼˜åŒ–é¡¾é—®ã€‚"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

const model = new ChatOpenAI({ temperature: 0.3 });
const memory = new ConversationBufferWindowMemory({
  k: 4, // ä»…æ³¨å…¥æœ€è¿‘ 4 æ¡æ¶ˆæ¯
  memoryKey: "history",
  returnMessages: true,
});

const chain = RunnableSequence.from([
  async (input: { input: string }) => ({
    input: input.input,
    history: (await memory.loadMemoryVariables({})).history,
  }),
  prompt,
  model,
  async (output) => { await memory.saveContext({}, { output }); return output; },
]);

export async function ask(q: string) {
  const res = await chain.invoke({ input: q });
  console.log("AI:", res);
}

if (require.main === module) {
  (async () => {
    await ask("é¡µé¢ç™½å±å¦‚ä½•æ’æŸ¥ï¼Ÿ");
    await ask("å¦‚ä½•ç”¨æ‡’åŠ è½½ä¼˜åŒ–é¦–å±ï¼Ÿ");
    await ask("å›¾ç‰‡è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ");
    await ask("å†è¯´è¯´éª¨æ¶å±ç­–ç•¥");
    await ask("å‰é¢æˆ‘ä»¬èŠè¿‡å“ªäº›ä¼˜åŒ–ç‚¹ï¼Ÿ"); // çª—å£å†…ä¿¡æ¯å¯è¢«å¼•ç”¨
  })();
}
```

### 3.7 Summaryï¼šç”¨æ‘˜è¦å‹ç¼©å†å²
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/summary.ts
import { ChatOpenAI } from "@langchain/openai";
import { ConversationSummaryMemory } from "langchain/memory";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";

const llm = new ChatOpenAI({ temperature: 0 });
const memory = new ConversationSummaryMemory({ llm: llm as any, memoryKey: "history" });

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ä¸¥è°¨çš„æŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹ï¼Œæ“…é•¿æ€»ç»“ä¸å¼•ç”¨ã€‚"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

const chain = RunnableSequence.from([
  async (input: { input: string }) => ({ ...input, history: await memory.loadMemoryVariables({}) }),
  prompt,
  llm,
  async (output) => { await memory.saveContext({}, { output }); return output; },
]);

export async function chat(q: string) {
  const res = await chain.invoke({ input: q });
  console.log(res);
}

if (require.main === module) {
  (async () => {
    await chat("è¯·æ€»ç»“æˆ‘ä»¬è¦åšçš„æ€§èƒ½ä¼˜åŒ–è·¯çº¿");
    await chat("é’ˆå¯¹å›¾ç‰‡å’Œè„šæœ¬åˆ†åˆ«ç»™å‡º 3 æ¡å»ºè®®");
    await chat("æŠŠæ€»ç»“æµ“ç¼©ä¸º 5 ä¸ªè¦ç‚¹");
  })();
}
```

### 3.8 VectorStoreRetrieverMemoryï¼šäº‹å®æ€§æŒä¹…è®°å¿†
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/vector-memory.ts
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { VectorStoreRetrieverMemory } from "langchain/memory"; // å¯èƒ½ä½äº @langchain/community

// ä¼ªå‘é‡æ£€ç´¢å™¨ï¼ˆå®é™…è¯·æ¥å…¥ Chroma/Pinecone/Weaviateï¼‰
const fakeRetriever = {
  async getRelevantDocuments(q: string) {
    return [{ pageContent: `ç”¨æˆ·åå¥½ï¼šæ›´å–œæ¬¢æš—è‰²ä¸»é¢˜ï¼›æœ€è¿‘å…³æ³¨â€œå“åº”å¼å¸ƒå±€â€ã€‚Q=${q}` } as any];
  }
};

const memory = new VectorStoreRetrieverMemory({
  retriever: fakeRetriever as any,
  memoryKey: "history", // å°†æ£€ç´¢ç»“æœå½“ä½œâ€œå†å²ä¸Šä¸‹æ–‡â€æ³¨å…¥
});

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ä¸ªæ€§åŒ– UI åŠ©æ‰‹ï¼Œå›ç­”éœ€è€ƒè™‘ç”¨æˆ·åå¥½ã€‚"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

const chain = RunnableSequence.from([
  async (input: { input: string }) => ({
    input: input.input,
    history: await memory.loadMemoryVariables({}),
  }),
  prompt,
  new ChatOpenAI({ temperature: 0 }),
]);

export async function advise(q: string) {
  const res = await chain.invoke({ input: q });
  console.log(res);
}

if (require.main === module) {
  advise("è¯·æ¨èé¦–é¡µå¸ƒå±€æ–¹æ¡ˆ");
}
```

### 3.9 è‡ªå®šä¹‰ Memoryï¼šç»Ÿä¸€æ¥å£
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/custom-memory.ts
import type { BaseChatMemory } from "langchain/memory";

type Message = { role: "human" | "ai" | "system"; content: string; ts?: number };

export class SimpleMemory implements BaseChatMemory {
  memoryKey = "history";
  private store: Record<string, Message[]> = {};

  constructor(private sessionId: string) {}

  get memoryKeys() { return [this.memoryKey]; }

  async loadMemoryVariables(_: any) {
    return { [this.memoryKey]: (this.store[this.sessionId] || []).map(m => ({ role: m.role, content: m.content })) };
  }

  async saveContext(input: any, output: any) {
    const arr = this.store[this.sessionId] || (this.store[this.sessionId] = []);
    if (input?.input) arr.push({ role: "human", content: input.input, ts: Date.now() });
    if (output?.content) arr.push({ role: "ai", content: output.content, ts: Date.now() });
  }

  async clear() { this.store[this.sessionId] = []; }
}
```

### 3.10 Redis/Mongo æŒä¹…åŒ–ï¼ˆç¤ºæ„ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/redis-memory.ts
import { createClient } from "redis";
import type { BaseChatMemory } from "langchain/memory";

export class RedisMemory implements BaseChatMemory {
  memoryKey = "history";
  constructor(private client = createClient(), private sessionId: string) {}

  async loadMemoryVariables() {
    const raw = await this.client.get(`mem:${this.sessionId}`);
    return { [this.memoryKey]: raw ? JSON.parse(raw) : [] };
  }

  async saveContext(input: any, output: any) {
    const prev = (await this.loadMemoryVariables())[this.memoryKey] as any[];
    const next = [...prev];
    if (input?.input) next.push({ role: "human", content: input.input, ts: Date.now() });
    if (output?.content) next.push({ role: "ai", content: output.content, ts: Date.now() });
    await this.client.set(`mem:${this.sessionId}`, JSON.stringify(next), { EX: 60 * 60 * 24 });
  }

  async clear() { await this.client.del(`mem:${this.sessionId}`); }
}
```

---

## ğŸ”— ä¸ Runnable/Callback/LangGraph é›†æˆï¼ˆçº¦ 10%ï¼‰

### 3.11 Runnableï¼šå¸¦ä¼šè¯çš„å¯å¤ç”¨é“¾
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/session-chain.ts
import { RunnableSequence } from "@langchain/core/runnables";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { SimpleMemory } from "./custom-memory";

export function createSessionChain(sessionId: string) {
  const memory = new SimpleMemory(sessionId);
  const prompt = ChatPromptTemplate.fromMessages([
    ["system", "ä½ æ˜¯ç¨³å¥çš„æŠ€æœ¯åŠ©æ‰‹ï¼Œé‡åˆ°ä¸ç¡®å®šè¯·å…ˆæé—®æ¾„æ¸…ã€‚"],
    new MessagesPlaceholder("history"),
    ["human", "{input}"],
  ]);

  return RunnableSequence.from([
    async (input: { input: string }) => ({ input: input.input, history: await memory.loadMemoryVariables({})["history"] }),
    prompt,
    new ChatOpenAI({ temperature: 0 }),
    new StringOutputParser(),
    async (out, input) => { await memory.saveContext(input, { content: out }); return out; },
  ]);
}
```

### 3.12 Callbackï¼šè§‚æµ‹è®°å¿†è¯»å†™
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/memory-callback.ts
import { ConsoleCallbackHandler } from "@langchain/core/callbacks/console";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";

const model = new ChatOpenAI({
  callbacks: [new ConsoleCallbackHandler()],
  verbose: true,
});

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ ä¼šåœ¨è¾“å‡ºä¸­æ ‡æ³¨ä½¿ç”¨åˆ°çš„å†å²è¦ç‚¹ã€‚"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

export async function run(history: any[], input: string) {
  const res = await prompt.pipe(model).invoke({ history, input });
  console.log(res.content);
}
```

### 3.13 LangGraphï¼šåœ¨çŠ¶æ€å›¾ä¸­ç®¡ç†è®°å¿†
```typescript
// æ–‡ä»¶ï¼šsrc/ch03/langgraph-memory.tsï¼ˆä¼ªç¤ºä¾‹ï¼Œæ¥å£ä»¥æœ¬åœ°ç‰ˆæœ¬ä¸ºå‡†ï¼‰
// å±•ç¤ºå¦‚ä½•æŠŠ memory æ”¾åœ¨å›¾çš„ state ä¸­ï¼Œè·¨èŠ‚ç‚¹å…±äº«
import { StateGraph } from "@langchain/langgraph";

type GraphState = {
  history: { role: string; content: string }[];
  input: string;
  output?: string;
};

const graph = new StateGraph<GraphState>({
  channels: {
    history: { value: [], default: [] },
    input: { value: "" },
    output: { value: "" },
  },
});

graph.addNode("llm", async (state) => {
  // è¯»å– state.history æ³¨å…¥åˆ° Promptï¼Œå†è°ƒç”¨ LLM
  // å¾—åˆ°è¾“å‡ºåå†™å› state.outputï¼Œå¹¶ push åˆ° history
  return { output: `å›ç­”:${state.input}`, history: [...state.history, { role: "ai", content: "..." }] };
});

graph.addEdge("start", "llm");
graph.addEdge("llm", "end");

export const app = graph.compile();
```

---

## ğŸ›¡ï¸ å¥å£®æ€§ä¸å®‰å…¨ï¼ˆçº¦ 5%ï¼‰

### 3.14 é”™è¯¯ä¸è¾¹ç•Œå¤„ç†
- ç»“æ„å¼‚å¸¸ï¼šMemory è¯»å†™å¤±è´¥ â†’ å›é€€åˆ°ç©ºå†å² + è®°å½•é”™è¯¯
- æ¶ˆæ¯å»é‡ï¼šå“ˆå¸Œæˆ–æŒ‡çº¹ï¼Œé¿å…é‡å¤æ³¨å…¥
- å†²çªå¤„ç†ï¼šåŒä¸€è½®å†…å¤šæ¬¡å†™å…¥æŒ‰æ—¶é—´æˆ³æ’åºã€å¹‚ç­‰åŒ–

### 3.15 éšç§ä¸åˆè§„
- æœ€å°åŒ–åŸåˆ™ï¼šä»…ä¿å­˜ä»»åŠ¡æ‰€éœ€çš„æœ€å°‘å†…å®¹
- æ•°æ®è„±æ•ï¼šPII/æ•æ„Ÿå­—æ®µè„±æ•æˆ–åªä¿ç•™æ‘˜è¦/å‘é‡
- å¯åˆ é™¤æƒï¼šæ”¯æŒä¼šè¯çº§æ¸…é™¤ã€ç”¨æˆ·çº§æ¸…é™¤
- è®¿é—®æ§åˆ¶ï¼šä¼šè¯éš”ç¦»ã€ç§Ÿæˆ·éš”ç¦»ã€å®¡è®¡æ—¥å¿—

---

## ğŸš€ å®æˆ˜é¡¹ç›®ä¸€ï¼šå¤šç”¨æˆ·ä¼šè¯ä¸­å¿ƒï¼ˆNext.js + Redisï¼‰ï¼ˆçº¦ 15%ï¼‰

### 3.16 ç›®æ ‡
- åœ¨ Next.js ä¸­å®ç° APIï¼šæ”¯æŒå¤šç”¨æˆ·å¤šä¼šè¯ï¼ŒRedis æŒä¹…åŒ– Memory
- è¦æ±‚ï¼šç§»åŠ¨ç«¯é€‚é…ã€å¯å–æ¶ˆæµå¼å“åº”ã€é”™è¯¯å…œåº•

### 3.17 æ•°æ®ç»“æ„
```ts
// ä¼šè¯ä¸»é”®ï¼šsession:{tenantId}:{userId}:{sessionId}
// å€¼ï¼š[{ role, content, ts }]
```

### 3.18 æ ¸å¿ƒæ¥å£ï¼ˆèŠ‚é€‰ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/pages/api/chat.tsï¼ˆç¤ºæ„ï¼‰
import type { NextApiRequest, NextApiResponse } from "next";
import { createClient } from "redis";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";

const client = createClient({ url: process.env.REDIS_URL });
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯å®¢æœåŠ©æ‰‹ï¼Œå›ç­”è¦ç®€æ´ä¸”å¼•ç”¨å†å²ã€‚"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const { tenantId, userId, sessionId, input } = req.body || {};
  const key = `session:${tenantId}:${userId}:${sessionId}`;

  const raw = (await client.get(key)) || "[]";
  const history = JSON.parse(raw);

  const model = new ChatOpenAI({ temperature: 0, streaming: true });
  const stream = await prompt.pipe(model).stream({ history, input });

  res.setHeader("Content-Type", "text/event-stream");
  for await (const chunk of stream) {
    res.write(`data: ${JSON.stringify(chunk)}\n\n`);
  }
  res.end();

  // å¼‚æ­¥å›å†™ï¼ˆçœç•¥é”™è¯¯å¤„ç†ä¸é™æµç»†èŠ‚ï¼‰
  history.push({ role: "human", content: input, ts: Date.now() });
  history.push({ role: "ai", content: "...æœ€ç»ˆèšåˆåçš„è¾“å‡º...", ts: Date.now() });
  await client.set(key, JSON.stringify(history), { EX: 60 * 60 * 24 * 7 });
}
```

### 3.19 å‰ç«¯è¦ç‚¹
- ç§»åŠ¨ç«¯ï¼šåº•éƒ¨è¾“å…¥æ å›ºå®šï¼Œé”®ç›˜é¡¶èµ·é˜²é®æŒ¡
- æµå¼ï¼šSSE/Fetch ReadableStream æ¸è¿›æ¸²æŸ“ï¼Œæä¾›â€œåœæ­¢/é‡è¯•â€
- é”™è¯¯ï¼šç½‘ç»œ/è¶…æ—¶ç»Ÿä¸€æç¤ºå¹¶å›é€€åˆ°æœ€åç¨³å®šçŠ¶æ€

---

## ğŸ§  å®æˆ˜é¡¹ç›®äºŒï¼šä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹ï¼ˆé•¿æœŸè®°å¿†ï¼‰ï¼ˆçº¦ 15%ï¼‰

### 3.20 ç›®æ ‡
- å°†â€œç”¨æˆ·åå¥½/çŸ¥è¯†å¡ç‰‡/æ˜“é”™ç‚¹â€ä»¥å‘é‡ä¸æ‘˜è¦åŒå­˜å‚¨
- å¯¹è¯æ—¶æŒ‰éœ€æ£€ç´¢ + æ€»ç»“æ³¨å…¥ï¼Œæé«˜ä¸ªæ€§åŒ–ä¸å¤ç”¨æ€§

### 3.21 å…³é”®æ¨¡å—
- Ingestï¼šä»å­¦ä¹ è®°å½•/ä¹¦ç­¾/é”™é¢˜é›†ç”Ÿæˆå‘é‡ä¸äº‹å®å¡
- Retrieverï¼šæŒ‰å½“å‰é—®é¢˜æ£€ç´¢ + å¬å›è¿‡æ»¤ + å»é‡
- Summarizerï¼šæ¯ 5-10 è½®ç”Ÿæˆâ€œé˜¶æ®µæ€§æ‘˜è¦â€å†™å›é•¿æœŸè®°å¿†
- Orchestratorï¼šRunnable/LangGraph è°ƒåº¦æµç¨‹

### 3.22 ä¼ªä»£ç ï¼ˆç¼–æ’ï¼‰
```typescript
const orchestrator = RunnableSequence.from([
  // 1) ä¸Šä¸‹æ–‡æ”¶é›†
  async (input: { q: string }) => ({ q: input.q, user: await loadUser() }),
  // 2) æ£€ç´¢é•¿æœŸè®°å¿†ï¼ˆå‘é‡ï¼‰
  async (ctx) => ({ ...ctx, facts: await vectorRetrieve(ctx.q, ctx.user.id) }),
  // 3) å†å²æ‘˜è¦ï¼ˆçŸ­æœŸâ†’é•¿æœŸå‹ç¼©ï¼‰
  async (ctx) => ({ ...ctx, summary: await loadOrUpdateSummary(ctx.user.id) }),
  // 4) Prompt ç»„è£…
  async (ctx) => ({
    prompt: buildPrompt({ q: ctx.q, facts: ctx.facts, summary: ctx.summary }),
  }),
  // 5) LLM & è§£æ
  async (ctx) => parse(await callLLM(ctx.prompt)),
  // 6) å›å†™ä¸æ‰“åˆ†
  async (out) => { await persist(out); return out; },
]);
```

### 3.23 è´¨é‡ä¸ä½“éªŒ
- ç»™å‡ºâ€œå¼•ç”¨â€ä¸â€œå­¦ä¹ è·¯å¾„å»ºè®®â€ï¼Œé™ä½å¹»è§‰
- ç§»åŠ¨ç«¯ï¼šå¡ç‰‡åŒ–å±•ç¤ºçŸ¥è¯†ç‚¹ã€å¯ä¸€é”®åŠ å…¥â€œè®°å¿†å¡ç‰‡â€
- ç›‘æ§ï¼šé•¿å¯¹è¯ token æˆæœ¬ä¸å“åº”æ—¶é—´æ›²çº¿

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶ï¼ˆçº¦ 5%ï¼‰

### 3.24 å»ºè®®
- çŸ­æœŸä½¿ç”¨ Windowï¼Œé•¿æœŸç”¨ Summary/Vector ç»“åˆ
- é˜¶æ®µæ€§æ‘˜è¦ï¼šæŒ‰å¯¹è¯è½®æ•°æˆ– token é˜ˆå€¼è§¦å‘
- æ£€ç´¢å‰è¿‡æ»¤ï¼šåŸºäºå…³é”®è¯/è§„åˆ™åˆç­›ï¼Œå‡å°‘å‘é‡æŸ¥è¯¢
- ç»“æœå»é‡ï¼šç›¸ä¼¼åº¦/æ ‡é¢˜æŒ‡çº¹ï¼Œé¿å…æ³¨å…¥é‡å¤
- ç¼“å­˜ï¼šä¼šè¯çº§ prompt æ¨¡æ¿ç¼“å­˜ã€æ£€ç´¢ç¼“å­˜

---

## ğŸ§ª æµ‹è¯•ä¸å¯è§‚æµ‹æ€§ï¼ˆçº¦ 5%ï¼‰

### 3.25 å›å½’é›†ä¸è¯„åˆ†
- æ„å»ºå¤šè½®å¯¹è¯ç”¨ä¾‹ï¼ŒåŒ…å«æ¾„æ¸…ã€çº é”™ã€å›å¿†å†å²ç­‰åœºæ™¯
- æŒ‡æ ‡ï¼šè¿ç»­æ€§ã€ä¸€è‡´æ€§ã€äº‹å®å‘½ä¸­ç‡ã€ç”¨æˆ·æ»¡æ„åº¦

### 3.26 å¯è§‚æµ‹
- Callback/æ—¥å¿—ï¼šè®°å½•æ³¨å…¥çš„å†å²æ¡æ•°ã€æ¥æºï¼ˆBuffer/Summary/Vectorï¼‰
- LangSmithï¼šé“¾è·¯å¯è§†åŒ–ã€token ä½¿ç”¨ã€é”™è¯¯ä¸å»¶è¿Ÿ
- è¿½è¸ª IDï¼šè´¯ç©¿å‰åç«¯ï¼Œä¾¿äºæ’é”™

---

## ğŸ“š å»¶ä¼¸èµ„æº
- LangChain.jsï¼ˆJSï¼‰æ–‡æ¡£ï¼š`https://js.langchain.com/`
- Memory æŒ‡å—ï¼ˆç¤¾åŒºï¼‰ï¼š`https://js.langchain.com/docs/modules/memory/`
- LangGraph çŠ¶æ€å›¾ï¼š`https://langchain-ai.github.io/langgraph/`
- Redis å®˜æ–¹æ–‡æ¡£ï¼š`https://redis.io/docs/latest/`
- ä¿æŠ¤éšç§ä¸åˆè§„ï¼šGDPR/CN ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ç­‰

---

## âœ… æœ¬ç« å°ç»“
- ç†è§£äº† 4 å¤§ç±»è®°å¿†çš„ä¼˜ç¼ºç‚¹ä¸ä½¿ç”¨è¾¹ç•Œ
- æŒæ¡äº† MessagesPlaceholder æ³¨å…¥ã€æ‘˜è¦å‹ç¼©ã€å‘é‡æ£€ç´¢
- å­¦ä¼šäº†æŒä¹…åŒ– Memoryã€å¹¶ä¸ Runnable/Callback/LangGraph åä½œ
- å®Œæˆâ€œå¤šç”¨æˆ·ä¼šè¯ä¸­å¿ƒâ€â€œä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹â€ä¸¤ç±»é¡¹ç›®çš„æ–¹æ¡ˆæ¼”ç»ƒ

## ğŸ¯ ä¸‹ç« é¢„å‘Š
ä¸‹ä¸€ç« ã€ŠCallback æœºåˆ¶ä¸äº‹ä»¶é©±åŠ¨æ¶æ„ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- æ„å»ºå¯è§‚æµ‹çš„é“¾è·¯æ—¥å¿—ä½“ç³»
- å®ç°æµå¼æ‰“å­—ã€è¿›åº¦æ¡ã€é”™è¯¯å‘Šè­¦
- ä¸å‰ç«¯å®æ—¶é€šä¿¡ï¼ˆSSE/WebSocketï¼‰å¯¹æ¥
- ç»“åˆ LangSmith åšé“¾è·¯è¿½è¸ªä¸ A/B è¯„æµ‹

> æœ€åæ„Ÿè°¢é˜…è¯»ï¼æ¬¢è¿å…³æ³¨æˆ‘ï¼Œå¾®ä¿¡å…¬ä¼—å·ï¼š`ã€Šé²«å°é±¼ä¸æ­£ç»ã€‹`ã€‚æ¬¢è¿ç‚¹èµã€æ”¶è—ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿ï¼ï¼ï¼

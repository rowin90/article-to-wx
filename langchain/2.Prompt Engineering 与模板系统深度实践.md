# ç¬¬2ç« ï¼šPrompt Engineering ä¸æ¨¡æ¿ç³»ç»Ÿæ·±åº¦å®è·µ

# å‰è¨€
å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯é²«å°é±¼ã€‚æ˜¯ä¸€å`ä¸å†™å‰ç«¯ä»£ç `çš„å‰ç«¯å·¥ç¨‹å¸ˆï¼Œçƒ­è¡·äºåˆ†äº«éå‰ç«¯çš„çŸ¥è¯†ï¼Œå¸¦é¢†åˆ‡å›¾ä»”é€ƒç¦»åˆ‡å›¾åœˆå­ï¼Œæ¬¢è¿å…³æ³¨æˆ‘ï¼Œå¾®ä¿¡å…¬ä¼—å·ï¼š`ã€Šé²«å°é±¼ä¸æ­£ç»ã€‹`ã€‚æ¬¢è¿ç‚¹èµã€æ”¶è—ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿ï¼ï¼

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡
- æŒæ¡ Prompt Engineering çš„æ ¸å¿ƒåŸåˆ™ä¸æ–¹æ³•è®ºï¼ˆæŒ‡ä»¤å¼ã€å°‘æ ·æœ¬ã€æ€ç»´é“¾ã€åæ€ç­‰ï¼‰
- ç†Ÿç»ƒä½¿ç”¨ LangChain.js çš„ Prompt æ¨¡æ¿ç³»ç»Ÿï¼ˆ`PromptTemplate`ã€`ChatPromptTemplate`ã€`FewShotPromptTemplate` ç­‰ï¼‰
- èƒ½å¤Ÿå°† Prompt ä¸ `Runnable`ã€`Callback`ã€`Memory`ã€`OutputParser` ç»„åˆä¸ºç¨³å®šçš„å¯å¤ç”¨é“¾è·¯
- å­¦ä¼šç»“æ„åŒ–è¾“å‡ºï¼ˆJSON/Zod Schemaï¼‰ä¸å¥å£®çš„é”™è¯¯å¤„ç†ç­–ç•¥
- å»ºç«‹ Prompt è¯„æµ‹ä¸ A/B æµ‹è¯•å·¥ä½œæµï¼ŒæŒæ¡è¿­ä»£ä¼˜åŒ–æ–¹æ³•
- é€šè¿‡ä¸¤ä¸ªå®æˆ˜é¡¹ç›®ï¼Œå®Œæˆä»ã€Œé—®é¢˜ â†’ è®¾è®¡ â†’ ç¼–ç  â†’ è¯„æµ‹ â†’ ä¸Šçº¿ã€çš„é—­ç¯

---

## ğŸ“– ç†è®ºåŸºç¡€ï¼šPrompt Engineering æ ¸å¿ƒç†å¿µï¼ˆçº¦ 30%ï¼‰

### 2.1 ä¸ºä»€ä¹ˆéœ€è¦ Prompt Engineering
- å¤§æ¨¡å‹å…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä½†è¾“å‡ºç¨³å®šæ€§å—ä¸Šä¸‹æ–‡ã€æç¤ºæªè¾ã€çº¦æŸæ¡ä»¶å½±å“æå¤§
- Prompt çš„è´¨é‡å†³å®šäº†ç»“æœçš„å¯é æ€§ã€å¯æ§æ€§ä¸æˆæœ¬ï¼ˆtokenï¼‰
- å·¥ç¨‹åŒ–çš„ Prompt å¯å¤ç”¨ã€å¯æµ‹è¯•ã€å¯ç›‘æ§ã€å¯æ¼”è¿›

### 2.2 åŸºæœ¬ç±»å‹ä¸ç­–ç•¥
- æŒ‡ä»¤å¼ï¼ˆInstructionï¼‰ï¼šæ¸…æ™°è§’è‰²ã€ä»»åŠ¡ã€çº¦æŸã€æ­¥éª¤
- å°‘æ ·æœ¬ï¼ˆFew-shotï¼‰ï¼šç¤ºä¾‹é©±åŠ¨ï¼Œé™ä½æ­§ä¹‰ï¼Œæé«˜é£æ ¼ä¸€è‡´æ€§
- æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼šæ˜¾å¼â€œå…ˆæ€è€ƒåä½œç­”â€ï¼Œæå‡æ¨ç†è´¨é‡
- åæ€ï¼ˆReflexionï¼‰ï¼šè¦æ±‚æ¨¡å‹è‡ªæ£€ä¸ä¿®æ­£ï¼Œé™ä½å¹»è§‰ç‡
- åˆ†è€Œæ²»ä¹‹ï¼ˆDecomposeï¼‰ï¼šå¤æ‚ä»»åŠ¡æ‹†è§£ä¸ºå­ä»»åŠ¡æµæ°´çº¿
- å·¥å…·åŒ–ï¼ˆTool-Useï¼‰ï¼šä¸å¤–éƒ¨å·¥å…·/æ£€ç´¢èåˆï¼Œæå‡äº‹å®æ€§

### 2.3 å¥½ Prompt çš„ 4 è¦ç´ ï¼ˆRICEï¼‰
- Roleï¼ˆè§’è‰²å®šä½ï¼‰ï¼šæ¨¡å‹æ‰®æ¼”è°ï¼ˆä¸“å®¶/å®¡æ ¡/äº§å“ç»ç†ï¼‰
- Instructionï¼ˆä»»åŠ¡æŒ‡ä»¤ï¼‰ï¼šåšä»€ä¹ˆã€äº§å‡ºä»€ä¹ˆæ ¼å¼
- Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰ï¼šå¿…è¦èƒŒæ™¯ã€çº¦æŸã€é£æ ¼ã€é¢†åŸŸè¯æ±‡
- Exampleï¼ˆç¤ºä¾‹ï¼‰ï¼šæ­£åç¤ºä¾‹ã€è¾¹ç•Œæ¡ˆä¾‹

### 2.4 å¯æµ‹è¯•ä¸å¯ç»´æŠ¤
- æ¨¡æ¿åŒ–ï¼šå˜é‡åŒ–å¯æ§è¾“å…¥ï¼Œä¾¿äºä¸åŒè°ƒç”¨åœºæ™¯å¤ç”¨
- ç»“æ„åŒ–è¾“å‡ºï¼šé¿å…çº¯è‡ªç„¶è¯­è¨€ï¼Œé™ä½è§£ææˆæœ¬
- è¯„æµ‹æŒ‡æ ‡ï¼šæ­£ç¡®ç‡ã€å¯è¯»æ€§ã€ä¸€è‡´æ€§ã€å¼•ç”¨ç‡ã€æˆæœ¬/æ—¶å»¶
- ç‰ˆæœ¬è¿­ä»£ï¼šPrompt ç‰ˆæœ¬å·ã€Changelogã€ç°åº¦ç­–ç•¥

---

## ğŸ§© LangChain.js Prompt æ¨¡æ¿ä½“ç³»ï¼ˆçº¦ 15%ï¼‰

### 2.5 å¸¸ç”¨ç±»ä¸èƒ½åŠ›
- `PromptTemplate`ï¼šæ–‡æœ¬æ¨¡æ¿ â†’ å¯æ³¨å…¥å˜é‡
- `ChatPromptTemplate`ï¼šæ¶ˆæ¯å¼æ¨¡æ¿ï¼ˆ`system`/`human`/`ai` ç­‰ï¼‰
- `FewShotPromptTemplate`ï¼šå°‘æ ·æœ¬ç¤ºä¾‹è‡ªåŠ¨æ‹¼æ¥
- `PipelinePromptTemplate`ï¼šå­æ¨¡æ¿ç®¡é“åŒ–ç»„åˆ
- `MessagesPlaceholder`ï¼šä¸ `Memory` åä½œï¼Œæ³¨å…¥å†å²å¯¹è¯
- `OutputParser`ï¼šæ­é… JSON/Zod è§£æä¸ºç»“æ„åŒ–å¯¹è±¡

### 2.6 æ¨¡æ¿è®¾è®¡å‡†åˆ™
- å•ä¸€èŒè´£ï¼šæ¯ä¸ªæ¨¡æ¿èšç„¦ä¸€ä¸ªç›®æ ‡
- è¾¹ç•Œæ¸…æ™°ï¼šè¯´æ˜è¾“å…¥å˜é‡ã€çº¦æŸã€è¾“å‡ºæ ¼å¼
- å¯è§‚æµ‹æ€§ï¼šä¸ºè¯„æµ‹ä¸è¿½è¸ªé¢„ç•™æ ‡è®°ï¼ˆå¦‚ versionã€taskIdï¼‰

---

## ğŸ’» åŸºç¡€ä»£ç å®è·µï¼ˆçº¦ 20%ï¼‰

### 2.7 æŒ‡ä»¤å¼ + å˜é‡æ³¨å…¥
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/basic-instruction.ts
import { PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";
import * as dotenv from "dotenv";
dotenv.config();

const prompt = PromptTemplate.fromTemplate(`
ä½ æ˜¯ä¸€ä¸ª{role}ã€‚è¯·ç”¨{style}é£æ ¼è§£ç­”ï¼š
é—®é¢˜ï¼š{question}
è¦æ±‚ï¼š
- ä½¿ç”¨åˆ†ç‚¹è¯´æ˜
- æ§åˆ¶åœ¨{maxTokens}å­—ä»¥å†…
- è‹¥ä¸ç¡®å®šï¼Œè¯·ç›´æ¥è¯´â€œæˆ‘éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡â€
`);

const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0.5 });
const chain = prompt.pipe(model).pipe(new StringOutputParser());

export async function run() {
  const result = await chain.invoke({
    role: "Web æ€§èƒ½ä¸“å®¶",
    style: "ç®€æ´åŠ¡å®",
    question: "å¦‚ä½•ä¼˜åŒ–é¦–å±æ¸²æŸ“ï¼Ÿ",
    maxTokens: 200,
  });
  console.log(result);
}

if (require.main === module) {
  run();
}
```

### 2.8 ChatPromptTemplate + å¤šæ¶ˆæ¯è§’è‰²
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/chat-prompt.ts
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";

const chatPrompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯èµ„æ·±å‰ç«¯æ•™ç»ƒï¼Œå–„äºç”¨ç±»æ¯”è§£é‡Šå¤æ‚æ¦‚å¿µã€‚"],
  ["human", "è¯·ç”¨ç±»æ¯”è§£é‡Š {topic}ï¼Œå¹¶æä¾›ä¸€ä¸ªä»£ç ç¤ºä¾‹ã€‚"],
]);

const model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });
const chain = chatPrompt.pipe(model).pipe(new StringOutputParser());

export async function run() {
  const text = await chain.invoke({ topic: "è™šæ‹Ÿ DOM" });
  console.log(text);
}

if (require.main === module) { run(); }
```

### 2.9 FewShotPromptTemplateï¼ˆå°‘æ ·æœ¬ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/few-shot.ts
import { FewShotPromptTemplate, PromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";

const examplePrompt = PromptTemplate.fromTemplate(
  "ç”¨æˆ·ï¼š{input}\nåˆ†ç±»ï¼š{label}"
);

const examples = [
  { input: "é¡µé¢åŠ è½½å¾ˆæ…¢", label: "æ€§èƒ½é—®é¢˜" },
  { input: "æŒ‰é’®ç‚¹å‡»æ²¡ååº”", label: "äº¤äº’ç¼ºé™·" },
  { input: "æ¥å£ç»å¸¸ 500", label: "åç«¯æ•…éšœ" },
];

const fewShot = new FewShotPromptTemplate({
  examples,
  examplePrompt,
  prefix: "è¯·æ ¹æ®ç”¨æˆ·è¯‰æ±‚ç»™å‡ºæ ‡ç­¾ï¼ˆæ€§èƒ½é—®é¢˜/äº¤äº’ç¼ºé™·/åç«¯æ•…éšœï¼‰ï¼š",
  suffix: "ç”¨æˆ·ï¼š{input}\nåˆ†ç±»ï¼š",
  inputVariables: ["input"],
});

const chain = fewShot.pipe(new ChatOpenAI()).pipe(new StringOutputParser());

export async function run() {
  const out = await chain.invoke({ input: "é¦–å±ç™½å± 3 ç§’" });
  console.log(out);
}

if (require.main === module) { run(); }
```

### 2.10 PipelinePromptTemplateï¼ˆç®¡é“æ¨¡æ¿ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/pipeline.ts
import { PromptTemplate, PipelinePromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { StringOutputParser } from "@langchain/core/output_parsers";

const partA = PromptTemplate.fromTemplate(
  "è¯·å°†ä¸»é¢˜æ‰©å†™ä¸º 3 ä¸ªå°æ ‡é¢˜ï¼š{topic}"
);

const partB = PromptTemplate.fromTemplate(
  "åŸºäºå°æ ‡é¢˜ç”Ÿæˆæçº²ï¼Œé£æ ¼ï¼š{style}\nå°æ ‡é¢˜ï¼š{headings}"
);

const pipeline = new PipelinePromptTemplate({
  finalPrompt: partB,
  pipelinePrompts: [
    { name: "headings", prompt: partA },
  ],
});

const chain = pipeline.pipe(new ChatOpenAI()).pipe(new StringOutputParser());

export async function run() {
  const result = await chain.invoke({ topic: "å‰ç«¯ç›‘æ§ç³»ç»Ÿ", style: "ä¸“ä¸šç®€ç»ƒ" });
  console.log(result);
}

if (require.main === module) { run(); }
```

---

## ğŸ§± ç»“æ„åŒ–è¾“å‡ºä¸ OutputParserï¼ˆçº¦ 10%ï¼‰

### 2.11 JSON è¾“å‡ºä¸ä¸¥æ ¼çº¦æŸ
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/json-output.ts
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { JsonOutputParser } from "@langchain/core/output_parsers";

type Plan = { steps: { title: string; details: string }[] };

const prompt = PromptTemplate.fromTemplate(`
ä½ æ˜¯é¡¹ç›®è§„åˆ’åŠ©æ‰‹ã€‚è¯·è¾“å‡ºä¸¥æ ¼çš„ JSONï¼š
{
  "steps": [
    { "title": string, "details": string }
  ]
}
ä¸»é¢˜ï¼š{topic}
`);

const model = new ChatOpenAI({ temperature: 0 });
const parser = new JsonOutputParser<Plan>();
const chain = prompt.pipe(model).pipe(parser);

export async function run() {
  const result = await chain.invoke({ topic: "å‰ç«¯ç›‘æ§å¹³å°æ­å»º" });
  console.log(result.steps.map(s => `- ${s.title}`).join("\n"));
}

if (require.main === module) { run(); }
```

### 2.12 Zod Schema å¼ºç±»å‹è§£æ
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/zod-output.ts
import { z } from "zod";
import { StructuredOutputParser } from "@langchain/core/output_parsers";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";

const schema = z.object({
  title: z.string(),
  tags: z.array(z.string()).max(5),
  estimateHours: z.number().min(1).max(80),
});

const parser = StructuredOutputParser.fromZodSchema(schema);

const prompt = PromptTemplate.fromTemplate(`
åŸºäºéœ€æ±‚ç”Ÿæˆä»»åŠ¡å¡ç‰‡ï¼š
éœ€æ±‚ï¼š{requirement}
è¯·ä¸¥æ ¼è¾“å‡ºï¼š
{format_instructions}
`);

const chain = prompt.pipe(new ChatOpenAI({ temperature: 0 })).pipe(parser);

export async function run() {
  const out = await chain.invoke({
    requirement: "å®ç°æ–‡ç« é˜…è¯»è¿›åº¦ç»Ÿè®¡ä¸æ”¶è—åŠŸèƒ½",
    format_instructions: parser.getFormatInstructions(),
  });
  console.log(out);
}

if (require.main === module) { run(); }
```

---

## ğŸ”— ä¸ Runnableã€Memoryã€Callback åä½œï¼ˆçº¦ 10%ï¼‰

### 2.13 Runnable ç»„åˆä¸å¤ç”¨
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/runnable-compose.ts
import { RunnableLambda, RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

const normalize = new RunnableLambda((input: { q: string }) => ({
  q: input.q.trim().slice(0, 300),
}));

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ä¸¥è°¨çš„æŠ€æœ¯ä½œå®¶ï¼Œè¾“å‡ºè§„èŒƒåŒ– Markdown"],
  ["human", "è¯·å›ç­”ï¼š{q}"],
]);

const chain = RunnableSequence.from([
  normalize,
  prompt,
  new ChatOpenAI({ temperature: 0.2 }),
  new StringOutputParser(),
]);

export async function run() {
  const md = await chain.invoke({ q: "  è®²è®²CSR/SSR/SSG åŒºåˆ«  " });
  console.log(md);
}

if (require.main === module) { run(); }
```

### 2.14 Memory æ³¨å…¥å†å²å¯¹è¯ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/memory-window.ts
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { ConversationSummaryMemory } from "langchain/memory"; // æˆ– @langchain/community ä¸­çš„è®°å¿†å®ç°
import { RunnableSequence } from "@langchain/core/runnables";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯æŒç»­å¯¹è¯çš„æŠ€æœ¯é¡¾é—®ï¼Œå›ç­”è¦ç®€æ´å¹¶å¼•ç”¨ä¸Šä¸‹æ–‡"],
  new MessagesPlaceholder("history"),
  ["human", "{input}"],
]);

const model = new ChatOpenAI({ temperature: 0 });
const memory = new ConversationSummaryMemory({ llm: model as any, memoryKey: "history" });

const chain = RunnableSequence.from([
  async (input: { input: string }) => ({ ...input, history: await memory.loadMemoryVariables({}) }),
  prompt,
  model,
  async (output) => { await memory.saveContext({}, { output }); return output; },
]);

export async function chatOnce(text: string) {
  const res = await chain.invoke({ input: text });
  console.log(res);
}

if (require.main === module) {
  (async () => {
    await chatOnce("æˆ‘ä»¬åˆšæ‰è®¨è®ºäº†å“ªäº›ä¼˜åŒ–ç‚¹ï¼Ÿ");
    await chatOnce("ç»§ç»­è¯´è¯´ CSS å±‚é¢çš„ä¼˜åŒ–ã€‚");
  })();
}
```

### 2.15 Callbackï¼šæ—¥å¿—ä¸æµå¼è§‚æµ‹
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/callbacks.ts
import { ChatOpenAI } from "@langchain/openai";
import { ConsoleCallbackHandler } from "@langchain/core/callbacks/console";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯è¾“å‡ºä¸¥æ ¼ JSON çš„åŠ©æ‰‹"],
  ["human", "ç»™å‡º 3 ä¸ªæ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼Œè¾“å‡º JSON æ•°ç»„ã€‚"],
]);

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  callbacks: [new ConsoleCallbackHandler()],
  verbose: true,
});

export async function run() {
  const res = await prompt.pipe(model).invoke({});
  console.log(res.content);
}

if (require.main === module) { run(); }
```

---

## ğŸ§ª Prompt è¯„æµ‹ä¸ A/B æµ‹è¯•ï¼ˆçº¦ 5%ï¼‰

### 2.16 è¯„æµ‹æ¸…å•ä¸æŒ‡æ ‡
- å‡†ç¡®æ€§ï¼ˆæ˜¯å¦å›ç­”æ­£é¢˜ï¼‰
- å¯è¯»æ€§ï¼ˆç»“æ„ã€æ ¼å¼ã€æœ¯è¯­å‹å¥½åº¦ï¼‰
- ä¸€è‡´æ€§ï¼ˆç›¸åŒè¾“å…¥çš„ç¨³å®šè¾“å‡ºï¼‰
- äº‹å®æ€§ï¼ˆæ˜¯å¦å¼•ç”¨å¯ä¿¡æ¥æºï¼ŒRAG ç»“åˆæ—¶å¼•ç”¨ç‡ï¼‰
- æˆæœ¬ä¸æ—¶å»¶ï¼ˆtoken ä½¿ç”¨ã€å“åº”æ—¶é—´ï¼‰

### 2.17 LangSmith ä¸å›å½’æ ·æœ¬é›†
- å»ºç«‹å›ºå®šé—®é¢˜é›†ï¼ˆgolden setï¼‰ï¼Œè¦†ç›–ä¸»æµç¨‹ä¸è¾¹ç•Œ
- ä¸ºæ¯æ¬¡ Prompt ç‰ˆæœ¬å˜æ›´åšå›å½’è¯„æµ‹
- æ”¶é›†çº¿ä¸ŠçœŸå®é—®é¢˜ï¼ŒæŒç»­è¡¥å……æ ·æœ¬

### 2.18 A/B å®æˆ˜æ ·ä¾‹ï¼ˆä¼ªä»£ç ï¼‰
```typescript
// A/B ä¸¤å¥— Prompt æ¨¡æ¿ï¼Œçº¿ä¸Šåˆ†æµ 20%/80%
// è®°å½•æ»¡æ„åº¦ã€è½¬åŒ–ç‡ã€äººå·¥æ ‡æ³¨å¾—åˆ†
```

---

## ğŸš€ å®æˆ˜é¡¹ç›®ä¸€ï¼šFAQ RAG Chatï¼ˆPrompt ä¸ºæ ¸å¿ƒï¼‰ï¼ˆçº¦ 15%ï¼‰

### 2.19 ç›®æ ‡
- ä¸ºäº§å“ FAQ/æ–‡æ¡£æä¾›é—®ç­”ï¼›è¦æ±‚ï¼šç»“æ„åŒ–ç­”æ¡ˆã€æ¥æºå¼•ç”¨ã€ä½å¹»è§‰
- ç”¨ Prompt è§„èŒƒåŒ–å›ç­”æ ¼å¼ï¼›ä¸å‘é‡æ£€ç´¢ï¼ˆVector/RAGï¼‰é…åˆ

### 2.20 é¡¹ç›®ç»“æ„
```
src/
  ch02/
    rag-faq/
      ingest.ts         # æ–‡æ¡£åŠ è½½ä¸å‘é‡åŒ–
      retriever.ts      # æ£€ç´¢å™¨
      prompt.ts         # å›ç­”æ¨¡æ¿ï¼ˆå«å¼•ç”¨ï¼‰
      answer.ts         # ç»„åˆé“¾è·¯
      server.ts         # API/CLI å…¥å£
```

### 2.21 å…³é”®ä»£ç ï¼ˆèŠ‚é€‰ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/rag-faq/prompt.ts
import { ChatPromptTemplate } from "@langchain/core/prompts";

export const answerPrompt = ChatPromptTemplate.fromMessages([
  ["system", `ä½ æ˜¯ä¸¥è°¨çš„ FAQ æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºâ€œæ£€ç´¢åˆ°çš„ç‰‡æ®µâ€å›ç­”ï¼Œè‹¥æ— ç­”æ¡ˆè¯·è¯´ä¸çŸ¥é“ã€‚
å¿…é¡»è¾“å‡ºä»¥ä¸‹ JSONï¼š
{
  "answer": string,
  "citations": [{"source": string, "chunkId": string}],
  "confidence": number // 0-1
}`],
  ["human", `ç”¨æˆ·é—®é¢˜ï¼š{question}
æ£€ç´¢ç‰‡æ®µï¼š\n{chunks}\nè¯·å›ç­”ã€‚`],
]);
```

```typescript
// æ–‡ä»¶ï¼šsrc/ch02/rag-faq/answer.ts
import { RunnableSequence } from "@langchain/core/runnables";
import { answerPrompt } from "./prompt";
import { ChatOpenAI } from "@langchain/openai";
import { JsonOutputParser } from "@langchain/core/output_parsers";

type QA = { answer: string; citations: { source: string; chunkId: string }[]; confidence: number };

export function buildQAChain(retriever: (q: string) => Promise<string>) {
  const model = new ChatOpenAI({ temperature: 0 });
  const parser = new JsonOutputParser<QA>();
  return RunnableSequence.from([
    async (input: { question: string }) => ({
      question: input.question,
      chunks: await retriever(input.question),
    }),
    answerPrompt,
    model,
    parser,
  ]);
}
```

```typescript
// æ–‡ä»¶ï¼šsrc/ch02/rag-faq/retriever.tsï¼ˆç¤ºæ„ï¼‰
// å®é™…å¯ç”¨ Chroma/Pinecone ç­‰å‘é‡åº“
export async function simpleRetriever(q: string): Promise<string> {
  return `# chunk-01 æ¥è‡ª docs/intro.md\nLangChain.js æ˜¯æ„å»º LLM åº”ç”¨çš„æ¡†æ¶...`;
}
```

### 2.22 Prompt è¦ç‚¹
- ä¸¥æ ¼ JSON è¾“å‡ºï¼Œä¾¿äºå‰ç«¯æ¸²æŸ“
- å«â€œè‹¥æ— ç­”æ¡ˆè¯·ç›´è¯´â€çš„é˜²å¹»è§‰æŠ¤æ 
- å¼•ç”¨åˆ—è¡¨ï¼ˆcitationsï¼‰å¼ºåˆ¶ç»“æ„
- å¼•å…¥ `confidence` ä¾¿äºæ’åº/è¿‡æ»¤

### 2.23 è¿è¡Œä¸è¯„æµ‹
- å»ºç«‹ 30+ å¸¸è§é—®é¢˜ä½œä¸º gold set
- A/B ä¸åŒè¯­æ°”/ç»“æ„æç¤ºï¼Œè§‚å¯Ÿç­”éæ‰€é—®ç‡
- è®°å½•å¼•ç”¨å‘½ä¸­ç‡ä¸äººå·¥æ ‡æ³¨åˆ†

---

## ğŸ› ï¸ å®æˆ˜é¡¹ç›®äºŒï¼šPrompt é©±åŠ¨çš„ã€Œéœ€æ±‚ â†’ ä»»åŠ¡å¡ç‰‡ã€ç”Ÿæˆå™¨ï¼ˆçº¦ 15%ï¼‰

### 2.24 ç›®æ ‡
- è¾“å…¥è‡ªç„¶è¯­è¨€éœ€æ±‚ï¼Œè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–ä»»åŠ¡å¡ï¼ˆæ ‡é¢˜ã€æ ‡ç­¾ã€é¢„ä¼°å·¥æ—¶ã€éªŒæ”¶æ ‡å‡†ï¼‰
- æ»¡è¶³ç»“æ„åŒ–è¾“å‡ºã€å¯å¤æ ¸ã€å¯å›å¡«çš„å·¥ç¨‹è¦æ±‚

### 2.25 é¡¹ç›®ç»“æ„
```
src/
  ch02/
    tasks/
      schema.ts
      prompt.ts
      chain.ts
      eval.ts
      cli.ts
```

### 2.26 æ ¸å¿ƒä»£ç ï¼ˆèŠ‚é€‰ï¼‰
```typescript
// æ–‡ä»¶ï¼šsrc/ch02/tasks/schema.ts
import { z } from "zod";
export const TaskSchema = z.object({
  title: z.string().min(6),
  tags: z.array(z.string()).max(5),
  estimateHours: z.number().min(1).max(80),
  acceptance: z.array(z.string()).min(1),
});
export type Task = z.infer<typeof TaskSchema>;
```

```typescript
// æ–‡ä»¶ï¼šsrc/ch02/tasks/prompt.ts
import { PromptTemplate } from "@langchain/core/prompts";
export const taskPrompt = PromptTemplate.fromTemplate(`
è¯·æŠŠä¸‹é¢éœ€æ±‚è½¬æˆä»»åŠ¡å¡ç‰‡ï¼ˆä¸¥æ ¼ JSONï¼‰ï¼š
éœ€æ±‚ï¼š{requirement}
è¿”å›ï¼š
{
  "title": string,
  "tags": string[],
  "estimateHours": number,
  "acceptance": string[]
}
`);
```

```typescript
// æ–‡ä»¶ï¼šsrc/ch02/tasks/chain.ts
import { ChatOpenAI } from "@langchain/openai";
import { StructuredOutputParser } from "@langchain/core/output_parsers";
import { TaskSchema } from "./schema";
import { taskPrompt } from "./prompt";

const parser = StructuredOutputParser.fromZodSchema(TaskSchema);
const model = new ChatOpenAI({ temperature: 0 });

export async function generateTask(requirement: string) {
  const res = await taskPrompt
    .pipe(model)
    .pipe(parser)
    .invoke({ requirement });
  return res;
}
```

```typescript
// æ–‡ä»¶ï¼šsrc/ch02/tasks/eval.ts  ï¼ˆç®€æ˜“è¯„æµ‹ï¼‰
import { generateTask } from "./chain";

const cases = [
  "ä¸ºåšå®¢å¢åŠ å…¨æ–‡æœç´¢ï¼Œæ”¯æŒæ ‡ç­¾è¿‡æ»¤å’Œé«˜äº®",
  "æ–°å¢å›¾ç‰‡ä¸Šä¼ ï¼Œè‡ªåŠ¨å‹ç¼©å¹¶ç”Ÿæˆ WebP",
];

(async () => {
  for (const c of cases) {
    const out = await generateTask(c);
    const ok = !!out.title && out.estimateHours > 0 && out.acceptance.length > 0;
    console.log("case:", c, ok ? "âœ…" : "âŒ", out);
  }
})();
```

### 2.27 è´¨é‡è¦ç‚¹
- ä¸¥æ ¼ Schema æ ¡éªŒï¼Œå¼‚å¸¸ç›´æ¥å¯è§
- å¤±è´¥æ ·ä¾‹å›æ”¶æˆå›å½’é›†ï¼ŒæŒç»­æ”¹è¿› Prompt
- å…è®¸å°‘é‡æ¸©åº¦ï¼Œä¿è¯å¤šæ ·æ€§ä½†ä¸è¿‡åº¦å‘æ•£

---

## âš™ï¸ æ€§èƒ½ã€æˆæœ¬ä¸å¥å£®æ€§ï¼ˆçº¦ 5%ï¼‰

### 2.28 ä¼˜åŒ–å»ºè®®
- æ¨¡æ¿å¤ç”¨ä¸ç¼“å­˜ï¼šé¿å…é‡å¤æ¸²æŸ“æ¨¡æ¿ä¸ä¸å¿…è¦çš„è°ƒç”¨
- é™åˆ¶è¾“å‡ºé•¿åº¦ä¸æ ¼å¼ï¼šé™ä½ token ä½¿ç”¨ä¸è§£ææˆæœ¬
- è¶…æ—¶/é‡è¯•/æŒ‡æ•°é€€é¿ï¼šåº”å¯¹ç½‘ç»œæŠ–åŠ¨ä¸ä¸´æ—¶é”™è¯¯
- å¤±è´¥é™çº§ï¼šä¸ºç©ºæ—¶è¿”å›å…œåº•æ–‡æ¡ˆæˆ–å¼•å¯¼æ”¶é›†æ›´å¤šä¸Šä¸‹æ–‡

### 2.29 å›é€€ç­–ç•¥ï¼ˆGuardrailsï¼‰
- ç»“æ„åŒ–è¾“å‡ºå¤±è´¥ â†’ è‡ªåŠ¨é‡è¯• + é™çº§çº¯æ–‡æœ¬ + é”™è¯¯ä¸ŠæŠ¥
- æ£€ç´¢ä¸ºç©º â†’ æç¤ºç»§ç»­æé—®æˆ–ç¼©å°èŒƒå›´
- æ•æ„Ÿ/è¶Šæƒé—®é¢˜ â†’ æ˜ç¡®æ‹’ç­”å¹¶ç»™å‡ºæ›¿ä»£å»ºè®®

---

## ğŸ” ä¸å‰ç«¯/äº§å“å·¥ç¨‹åä½œè¦ç‚¹ï¼ˆçº¦ 5%ï¼‰

### 2.30 å‰ç«¯é›†æˆ
- æµå¼è¾“å‡ºï¼šæ‰“å­—æœºæ•ˆæœã€å–æ¶ˆ/é‡è¯•æŒ‰é’®
- ç»“æ„åŒ–æ¸²æŸ“ï¼šåŸºäº JSON ç›´æ¥æ¸²æŸ“å¡ç‰‡/è¡¨æ ¼/å¼•ç”¨
- é”™è¯¯å…œåº•ï¼šå¯è§†åŒ–é”™è¯¯æç¤ºä¸é‡è¯•å¼•å¯¼

### 2.31 äº§å“è½åœ°
- æŒ‡æ ‡çœ‹æ¿ï¼šæ»¡æ„åº¦ã€äººå·¥æ¥ç®¡ç‡ã€æ‹’ç­”ç‡ã€æˆæœ¬
- ç‰ˆæœ¬åˆ‡æ¢ï¼šPrompt ç‰ˆæœ¬ç°åº¦ä¸å¿«é€Ÿå›æ»š
- åˆè§„å®‰å…¨ï¼šæ•æ„Ÿè¯æ²»ç†ä¸æ•°æ®æœ€å°åŒ–

---

## ğŸ“š èµ„æºä¸å»¶ä¼¸
- LangChain.js æ–‡æ¡£ï¼ˆJSï¼‰ï¼š`https://js.langchain.com/`
- LangGraphï¼š`https://langchain-ai.github.io/langgraph/`
- LangSmithï¼š`https://docs.smith.langchain.com/`
- æç¤ºå·¥ç¨‹æœ€ä½³å®è·µåˆé›†ï¼š`https://learnprompting.org/`
- OpenAI æŒ‡å—ï¼š`https://platform.openai.com/docs/guides/prompt-engineering`

---

## ğŸ“¦ é™„ï¼šç¤ºä¾‹é¡¹ç›®æœ€å°å¯è¿è¡Œæ¸…å•
```bash
mkdir -p src/ch02 && cd $_
# å°†ä¸Šè¿° *.ts æ–‡ä»¶æ”¾å…¥å¯¹åº”ç›®å½•åï¼š
npm i @langchain/core @langchain/openai @langchain/community zod dotenv
npm i -D tsx typescript @types/node
echo '{
  "compilerOptions": {"target":"ES2020","module":"commonjs","esModuleInterop":true,"strict":true},
  "include": ["src/**/*"]
}' > tsconfig.json
```

```json
// package.json ç‰‡æ®µ
{
  "scripts": {
    "ch02:basic": "tsx src/ch02/basic-instruction.ts",
    "ch02:chat": "tsx src/ch02/chat-prompt.ts",
    "ch02:few": "tsx src/ch02/few-shot.ts",
    "ch02:pipe": "tsx src/ch02/pipeline.ts",
    "ch02:json": "tsx src/ch02/json-output.ts",
    "ch02:zod": "tsx src/ch02/zod-output.ts"
  }
}
```

---

## âœ… æœ¬ç« å°ç»“
- ç³»ç»ŸåŒ–æŒæ¡äº† Prompt çš„æ ¸å¿ƒæ–¹æ³•ä¸å·¥ç¨‹åŒ–è½åœ°
- ç†Ÿç»ƒè¿ç”¨ LangChain.js æ¨¡æ¿ä½“ç³»ä¸ç»“æ„åŒ–è¾“å‡º
- å­¦ä¼šå°† Prompt ä¸ Runnable/Memory/Callback ç»„åˆ
- å»ºç«‹ Prompt è¯„æµ‹ä¸ A/B æµ‹è¯•çš„è¿­ä»£é—­ç¯
- å®Œæˆä¸¤ä¸ªå®æˆ˜é¡¹ç›®ï¼Œä» 0 åˆ° 1 èµ°é€šäº§å“åŒ–è·¯å¾„

## ğŸ¯ ä¸‹ç« é¢„å‘Š
ä¸‹ä¸€ç« ã€ŠMemory ç³»ç»Ÿä¸å¯¹è¯çŠ¶æ€ç®¡ç†ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥ï¼š
- çŸ­æœŸ/é•¿æœŸè®°å¿†ç­–ç•¥çš„æƒè¡¡
- æ»‘åŠ¨çª—å£ä¸æ‘˜è¦è®°å¿†çš„ç»„åˆ
- ä¸å‘é‡è®°å¿†ï¼ˆVectorStoreï¼‰ååŒï¼Œæ‰“é€ å¯æ£€ç´¢çš„å¯¹è¯ç³»ç»Ÿ
- è®°å¿†çš„ä¸€è‡´æ€§ã€æˆæœ¬ä¸éšç§å®‰å…¨

> æœ€åæ„Ÿè°¢é˜…è¯»ï¼æ¬¢è¿å…³æ³¨æˆ‘ï¼Œå¾®ä¿¡å…¬ä¼—å·ï¼š`ã€Šé²«å°é±¼ä¸æ­£ç»ã€‹`ã€‚æ¬¢è¿ç‚¹èµã€æ”¶è—ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿ï¼ï¼ï¼
